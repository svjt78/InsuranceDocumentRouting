# Insurance Document Management System ................................................................... 1
## Features ................................................................................................................ 1
## Technologies Used ................................................................................................. 2
## Architecture ........................................................................................................... 3
## Setup and Installation ............................................................................................ 3
### Prerequisites ........................................................................................................ 3
### Getting Started ..................................................................................................... 3
1. **Clone the Repository:** ................................................................................... 4
2. **Project Structure:** ......................................................................................... 4
3. **Environment Configuration (`.env`):** .............................................................. 5
4. **Run with Docker Compose:** ........................................................................... 6
5. **Access the Services:** ..................................................................................... 6
## Configuration ......................................................................................................... 6
## Contributing and Future Enhancements .................................................................. 7

This project is an AI-powered application designed to manage insurance documents. It
automates the processing of uploaded documents, including classification,
summarization, and action item recommendation, and provides a dashboard for
oversight and manual correction.

## Features
* **Document Upload:** Upload insurance documents either via a dedicated API
endpoint or through system integrations.
* **Automated Processing:** Documents undergo Optical Character Recognition
(OCR), LLM-based classification, summarization, and action item extraction.
* **Document Storage:** Uploaded documents are stored in MinIO object storage.
* **Metadata Management:** Document metadata, extracted text, and processing
results are stored in a PostgreSQL database.
* **Hierarchical Classification:** Documents are classified into a predefined
organizational hierarchy (Department, Category, Subcategory).
* **Dashboard Overview:** A user-friendly dashboard (built with Next.js and
TailwindCSS) provides oversight of documents.
* **Document Listing & Detail View:** View a list of all processed documents and
detailed information for each.
* **Manual Classification Override:** Users can manually correct or override the
automated classification results via the dashboard.
* **Metrics Dashboard:** View key metrics such as document status breakdown, daily
processing volume, backlog, average latency, override rate, and reroute success rate.
* **Organizational Hierarchy Visualization:** View the document classification
hierarchy (Department, Category, Subcategory) on the dashboard, possibly as an
interactive org chart.
* **Real-time Updates:** The dashboard receives real-time updates on document
status changes via WebSockets.
* **Configuration Management:** Manage settings like bucket mappings and email
settings via dedicated endpoints/sections.

## Technologies Used
**Backend (FastAPI)**:
* **Framework:** FastAPI (Python)
* **Database:** PostgreSQL with SQLAlchemy (ORM)
* **Object Storage:** MinIO (S3-compatible) with boto3 client
* **Message Queue:** RabbitMQ with pika (or aio-pika)
* **OCR:** Tesseract + OpenCV
* **LLM Integration:** OpenAI API (or other models)
* **Dependency Management:** `requirements.txt` and pip
* **Web Server:** Uvicorn
* **Logging:** Standard Python logging
* **Configuration:** Environment variables managed via `.env` and `os.getenv`
**Frontend (Next.js)**:
* **Framework:** Next.js (React)
* **Styling:** TailwindCSS
* **State Management/Data Fetching:** Data integrated from FastAPI backend via
REST endpoints.
* **Real-time Communication:** WebSockets for live updates.
**Containerization & Orchestration:**
* **Development:** Docker Compose
* **Production (Future):** Kubernetes
**Other Tools:**
* **Development Environment:** VS Code
* **Version Control:** Git/GitHub
* **API Documentation:** Swagger UI (auto-generated by FastAPI)
  
## Architecture
The application follows a microservice-like architecture. The core components include:
* **FastAPI Backend:** Handles API requests (uploads, document retrieval, overrides),
interacts with the database and storage, and publishes messages to the queue.
* **Workers (OCR, Classification, PII Masking):** Separate logical components (initially
part of the backend service in the provided structure) that consume messages from
RabbitMQ to perform processing tasks.
* **PostgreSQL Database:** Stores document metadata, classification results,
summaries, action items, and configuration data (like bucket mappings and hierarchy).
* **MinIO Storage:** Stores the raw document files.
* **RabbitMQ Message Queue:** Decouples the upload/ingestion process from the
processing workers.
* **Next.js Frontend (Dashboard):** Provides the user interface for uploading, viewing,
managing documents, and seeing metrics.
The flow typically involves a user uploading a document via the `/upload` endpoint or via
system integrations, the backend saving it to MinIO and creating a database record, and
then publishing a message to RabbitMQ to trigger the processing OCR workers for
information extraction, classification and processing.

## Setup and Installation
These instructions guide you through setting up and running the project locally using
Docker Compose.

### Prerequisites
* Docker and Docker Compose
* Python 3.9+ (for local development outside Docker, if needed)
* Node.js (for local frontend development outside Docker, if needed)
* VS Code (Recommended IDE)
* 
### Getting Started
1. **Clone the Repository:**
```bash
git clone https://github.com/svjt78/InsuranceDocumentRouting.git
cd InsDocRouting
```
2. **Project Structure:**
Ensure your project follows a structure similar to this:
```
project-root/
├── backend/
│ ├── app/
│ │ ├── __init__.py
│ │ ├── main.py
│ │ ├── config.py
│ │ ├── database.py
│ │ ├── models.py
│ │ ├── rabbitmq.py
│ │ ├── ocr_worker.py # Worker logic (could be separate service)
│ │ ├── llm_classifier.py # Worker logic (could be separate service)
│ │ ├── pii_masker.py # Worker logic (could be separate service)
│ │ ├── bucket_mappings.py # Backend endpoint/logic
│ │ ├── email_settings.py # Backend endpoint/logic
│ │ ├── seed_data/ # Data seeding scripts/files
│ │ │ ├── __init__.py
│ │ │ ├── seed_hierarchy.py
│ │ │ └── doc_hierarchy.json
│ │ ├── routes/ # Separate routers (e.g., doc_hierarchy)
│ │ │ ├── __init__.py
│ │ │ └── doc_hierarchy.py
│ │ ├── metrics/ # Metrics module
│ │ │ ├── __init__.py
│ │ │ ├── router.py
│ │ │ ├── service.py
│ │ │ └── schemas.py
│ │ └── logging_config.py # If separate logging setup used
│ ├── requirements.txt
│ └── Dockerfile # or Dockerfile.dev, Dockerfile.prod etc.
├── frontend/
│ ├── public/
│ ├── src/
│ │ ├── pages/
│ │ ├── components/
│ │ └── styles/
│ ├── package.json
│ └── Dockerfile
├── .env # Contains all environment variables
├── docker-compose.yml
└── .gitignore # Prevents .env and other sensitive files from being committed
```
3. **Environment Configuration (`.env`):**
Create a file named `.env` in the **project root** directory (at the same level as
`docker-compose.yml` and the `backend` and `frontend` folders). Populate it with the
necessary variables.
```env
# === DATABASE CONFIG ===
POSTGRES_USER=user # User for the PostgreSQL service within Docker
POSTGRES_PASSWORD=pass # Password for the PostgreSQL service within
Docker
POSTGRES_DB=db_name # Database name for the PostgreSQL service within
Docker
DATABASE_URL=postgresql://user:pass@db:5432/db_name # Connection URL for
the backend
# === RABBITMQ CONFIG ===
RABBITMQ_URL=amqp://rabbitmq:5672/ # Connection URL for the backend
# === MINIO CONFIG ===
MINIO_URL=http://minio:9000 # Connection URL for the backend
AWS_ACCESS_KEY_ID=minioadmin # Access Key for backend to connect to MinIO
(using boto3 naming)
AWS_SECRET_ACCESS_KEY=minioadmin # Secret Key for backend to connect to
MinIO (using boto3 naming)
MINIO_ROOT_USER=minioadmin # Root user for the MinIO service itself
MINIO_ROOT_PASSWORD=minioadmin # Root password for the MinIO service
itself
MINIO_BUCKET=documents # Default bucket name
# === OPENAI CONFIG ===
OPENAI_API_KEY=your_openai_api_key_here # Replace with your OpenAI API key
# === TESSERACT CONFIG ===
TESSERACT_CMD=/usr/bin/tesseract # Command for Tesseract (likely this path
within the Docker container)
# === FRONTEND CONFIG ===
# URL for the frontend to connect to the backend API. Use the service name
'backend'
# if the frontend is running inside the same Docker network, or localhost if run
outside.
NEXT_PUBLIC_API_URL=http://backend:8000 # Use http://localhost:8000 if frontend
is not in Docker
```
**Note:** Ensure `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`
are present and match the MinIO credentials used by the backend service. The
`MINIO_ROOT_USER` and `MINIO_ROOT_PASSWORD` are used by the MinIO
service container itself.
4. **Run with Docker Compose:**
Navigate to the project root directory in your terminal and run:
```bash
docker-compose down -v --remove-orphans # Clean up previous runs and volumes
docker-compose build --no-cache # Rebuild all services
docker-compose up # Start all services
```
*(Optional: Add `-d` to `docker-compose up` to run in detached mode)*.
5. **Access the Services:**
Once the services are running, you can access them at the following URLs:
* **Frontend Dashboard:** `http://localhost:3000` (or `http://localhost:3001` if
configured to run on port 3001)
* **Backend API Documentation (Swagger UI):** `http://localhost:8000/docs`
* **RabbitMQ Management Dashboard:** `http://localhost:15672` (Default
guest/guest credentials)
* **MinIO Console:** `http://localhost:9000` (or `http://localhost:9001` if configured
with `--console-address`) (Use the credentials from your `.env` file, e.g.,
`minioadmin`/`minioadmin`)
On startup, the backend will automatically create the necessary database tables and
seed the document hierarchy if the table is empty, and ensure the MinIO bucket exists.

## Configuration
All application configuration is managed via environment variables, typically loaded from
the `.env` file at the project root.
* Database credentials and connection URL.
* RabbitMQ connection URL.
* MinIO endpoint, access key, secret key, root user/password, and bucket name. Note
the distinction between keys used by the backend client (`AWS_ACCESS_KEY_ID`,
`AWS_SECRET_ACCESS_KEY`) and credentials used by the MinIO service itself
(`MINIO_ROOT_USER`, `MINIO_ROOT_PASSWORD`).
* OpenAI API key.
* Tesseract command path.
* Frontend backend API URL.
* CORS origins are configured in `backend/app/main.py`.
Sensitive information like API keys and passwords should **only** be stored in the
`.env` file and **never** committed to version control. Ensure your `.gitignore` file
prevents this.

## Contributing and Future Enhancements
This project provides a solid foundation. Potential areas for contribution and future
enhancements include:
* Implementing the LLM training/feedback loop leveraging human feedback from the
dashboard.
* Mobile responsiveness
* Implementing the WebSocket real-time updates in the frontend.
* Adding more robust error handling and logging.
* Implementing Single Sign-On (SSO) for authentication.
* Transitioning to Kubernetes for production deployment.
* Setting up Continuous Integration/Continuous Deployment (CI/CD) pipelines.
* Adding integration tests.
* Implementing async DB sessions for improved performance.
Feel free to fork the repository, open issues, or submit pull requests.
